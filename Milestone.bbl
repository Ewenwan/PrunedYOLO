% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.6 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \sortlist[entry]{ynt/global}
    \entry{V.Oseledets2011}{article}{}
      \name{author}{1}{}{%
        {{hash=58af900bda3051256f58892bae644aa7}{%
           family={{V.\bibnamedelimi Oseledets}},
           family_i={V\bibinitperiod}}}%
      }
      \strng{namehash}{58af900bda3051256f58892bae644aa7}
      \strng{fullhash}{58af900bda3051256f58892bae644aa7}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprinttype}{arXiv}
      \field{isbn}{0001405101}
      \field{journaltitle}{SIAM J. Scientific Computing}
      \field{number}{5}
      \field{title}{{Tensor-Train Decomposition}}
      \field{volume}{33}
      \field{year}{2011}
      \field{pages}{1948\bibrangedash 1974}
      \range{pages}{27}
      \verb{doi}
      \verb 10.1137/09076756X
      \endverb
      \verb{eprint}
      \verb arXiv:1302.5877
      \endverb
      \verb{file}
      \verb :C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Tensor Train Decomposition.pdf:pdf
      \endverb
      \keyw{090761811,10,1137,62k20,62l05,62p30,65d05,active learning,ams subject classifications,doi,experimental design,local linear approximation,nonlinear function approximation,sequential design}
    \endentry
    \entry{KITTIVision}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=06133052ac3c14a188601b82783cd4f7}{%
           family={Geiger},
           family_i={G\bibinitperiod},
           given={Andreas},
           given_i={A\bibinitperiod}}}%
        {{hash=f594289a77a4b801a7156edc29235ad8}{%
           family={Lenz},
           family_i={L\bibinitperiod},
           given={Philip},
           given_i={P\bibinitperiod}}}%
        {{hash=bc58c1e920023b071a4d0695299cf804}{%
           family={Urtasun},
           family_i={U\bibinitperiod},
           given={Raquel},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{627edf12ee4004074265a942326edc30}
      \strng{fullhash}{627edf12ee4004074265a942326edc30}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Conference on Computer Vision and Pattern	Recognition (CVPR)}
      \field{title}{Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite}
      \field{year}{2012}
    \endentry
    \entry{KITTIDataset}{article}{}
      \name{author}{4}{}{%
        {{hash=06133052ac3c14a188601b82783cd4f7}{%
           family={Geiger},
           family_i={G\bibinitperiod},
           given={Andreas},
           given_i={A\bibinitperiod}}}%
        {{hash=f594289a77a4b801a7156edc29235ad8}{%
           family={Lenz},
           family_i={L\bibinitperiod},
           given={Philip},
           given_i={P\bibinitperiod}}}%
        {{hash=ca1de2d211c8e4a0f976b3b90a625cb8}{%
           family={Stiller},
           family_i={S\bibinitperiod},
           given={Christoph},
           given_i={C\bibinitperiod}}}%
        {{hash=bc58c1e920023b071a4d0695299cf804}{%
           family={Urtasun},
           family_i={U\bibinitperiod},
           given={Raquel},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{f6509cc8b735c60c2b3a339b6dc468e9}
      \strng{fullhash}{5a983328f9e192b0dde84b863e9f7f4e}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{International Journal of Robotics Research (IJRR)}
      \field{title}{Vision meets Robotics: The KITTI Dataset}
      \field{year}{2013}
    \endentry
    \entry{Girshick2015}{article}{}
      \name{author}{1}{}{%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           family_i={G\bibinitperiod},
           given={Ross},
           given_i={R\bibinitperiod}}}%
      }
      \strng{namehash}{bd5dadbe57bedc5957c19a3154c4d424}
      \strng{fullhash}{bd5dadbe57bedc5957c19a3154c4d424}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781467383912}
      \field{issn}{15505499}
      \field{journaltitle}{Proceedings of the IEEE International Conference on Computer Vision}
      \field{title}{{Fast R-CNN}}
      \field{volume}{2015 Inter}
      \field{year}{2015}
      \field{pages}{1440\bibrangedash 1448}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/ICCV.2015.169
      \endverb
      \verb{eprint}
      \verb 1504.08083
      \endverb
      \verb{file}
      \verb :C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Fast{\_}RCNN.pdf:pdf
      \endverb
    \endentry
    \entry{Han2015}{article}{}
      \name{author}{3}{}{%
        {{hash=873e4e07ff73d563d2651d20ffaf99e0}{%
           family={Han},
           family_i={H\bibinitperiod},
           given={Song},
           given_i={S\bibinitperiod}}}%
        {{hash=1341c7c7f8d6acc2c0c244d6526ff658}{%
           family={Mao},
           family_i={M\bibinitperiod},
           given={Huizi},
           given_i={H\bibinitperiod}}}%
        {{hash=0ea138b81cd15c0a1081231fedfaf445}{%
           family={Dally},
           family_i={D\bibinitperiod},
           given={William\bibnamedelima J.},
           given_i={W\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{2928a4071224572314d9c0e28732e0b0}
      \strng{fullhash}{2928a4071224572314d9c0e28732e0b0}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.}
      \field{eprinttype}{arXiv}
      \field{title}{{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}}
      \field{year}{2015}
      \field{pages}{1\bibrangedash 14}
      \range{pages}{14}
      \verb{doi}
      \verb abs/1510.00149/1510.00149
      \endverb
      \verb{eprint}
      \verb 1510.00149
      \endverb
      \verb{file}
      \verb :C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Han Mao Dally 2016.pdf:pdf
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1510.00149
      \endverb
    \endentry
    \entry{Novikov2015}{article}{}
      \name{author}{4}{}{%
        {{hash=7bdd145d92cfa3763c41441779b1d37d}{%
           family={Novikov},
           family_i={N\bibinitperiod},
           given={Alexander},
           given_i={A\bibinitperiod}}}%
        {{hash=746086ff4225c386683e93a2c956a39e}{%
           family={Podoprikhin},
           family_i={P\bibinitperiod},
           given={Dmitry},
           given_i={D\bibinitperiod}}}%
        {{hash=1cada5b23b77bf3795eb342f76d5f3c5}{%
           family={Osokin},
           family_i={O\bibinitperiod},
           given={Anton},
           given_i={A\bibinitperiod}}}%
        {{hash=f1c8fe7ad4d22499bdaaa3917e11783d}{%
           family={Vetrov},
           family_i={V\bibinitperiod},
           given={Dmitry},
           given_i={D\bibinitperiod}}}%
      }
      \strng{namehash}{6c1c85031c2c8410168d7ec18104a290}
      \strng{fullhash}{44449e56f91f1e64f706c6db8c47e3ed}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.}
      \field{eprinttype}{arXiv}
      \field{issn}{10495258}
      \field{title}{{Tensorizing Neural Networks}}
      \field{year}{2015}
      \field{pages}{1\bibrangedash 9}
      \range{pages}{9}
      \verb{eprint}
      \verb 1509.06569
      \endverb
      \verb{file}
      \verb :C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Tensorizing Neural Networks.pdf:pdf
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1509.06569
      \endverb
    \endentry
    \entry{Redmon2015}{article}{}
      \name{author}{4}{}{%
        {{hash=99bced2e56a5253f3fe98a5f04e6d9b2}{%
           family={Redmon},
           family_i={R\bibinitperiod},
           given={Joseph},
           given_i={J\bibinitperiod}}}%
        {{hash=05ca9f19da9ecbd2def4e5514f8043c8}{%
           family={Divvala},
           family_i={D\bibinitperiod},
           given={Santosh},
           given_i={S\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           family_i={G\bibinitperiod},
           given={Ross},
           given_i={R\bibinitperiod}}}%
        {{hash=396c6ddedb6f986906fc3e4994d19974}{%
           family={Farhadi},
           family_i={F\bibinitperiod},
           given={Ali},
           given_i={A\bibinitperiod}}}%
      }
      \strng{namehash}{fceadd5e0c88bc623c502d7999f124d7}
      \strng{fullhash}{b5530443e433a4da53dbe3cf155225b4}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.}
      \field{eprinttype}{arXiv}
      \field{isbn}{978-1-4673-8851-1}
      \field{issn}{01689002}
      \field{title}{{You Only Look Once: Unified, Real-Time Object Detection}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.1109/CVPR.2016.91
      \endverb
      \verb{eprint}
      \verb 1506.02640
      \endverb
      \verb{file}
      \verb :C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/YOLO.pdf:pdf
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1506.02640
      \endverb
    \endentry
    \entry{Liu2016}{article}{}
      \name{author}{7}{}{%
        {{hash=c0e0d23e2d09e45e6f51cc2bcea6d9f9}{%
           family={Liu},
           family_i={L\bibinitperiod},
           given={Wei},
           given_i={W\bibinitperiod}}}%
        {{hash=c1826f3465579186aff299a9b0e16ed7}{%
           family={Anguelov},
           family_i={A\bibinitperiod},
           given={Dragomir},
           given_i={D\bibinitperiod}}}%
        {{hash=8bbc4c5d96f205bada839e74e0202146}{%
           family={Erhan},
           family_i={E\bibinitperiod},
           given={Dumitru},
           given_i={D\bibinitperiod}}}%
        {{hash=ed568d9c3bb059e6bf22899fbf170f86}{%
           family={Szegedy},
           family_i={S\bibinitperiod},
           given={Christian},
           given_i={C\bibinitperiod}}}%
        {{hash=698ee61a2f3fa29734204496d2d36aef}{%
           family={Reed},
           family_i={R\bibinitperiod},
           given={Scott},
           given_i={S\bibinitperiod}}}%
        {{hash=13ac13167fdca4e74c3347a2bcec0de4}{%
           family={Fu},
           family_i={F\bibinitperiod},
           given={Cheng\bibnamedelima Yang},
           given_i={C\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=ad327617ae56ac5e0522568c50eed9c8}{%
           family={Berg},
           family_i={B\bibinitperiod},
           given={Alexander\bibnamedelima C.},
           given_i={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{d4145b4ad8209d9eefd91697296598f5}
      \strng{fullhash}{4ea21ff867c45e21d4679afa82c48b13}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .}
      \field{eprinttype}{arXiv}
      \field{isbn}{9783319464473}
      \field{issn}{16113349}
      \field{journaltitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{title}{{SSD: Single shot multibox detector}}
      \field{volume}{9905 LNCS}
      \field{year}{2016}
      \field{pages}{21\bibrangedash 37}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/978-3-319-46448-0_2
      \endverb
      \verb{eprint}
      \verb 1512.02325
      \endverb
      \verb{file}
      \verb :C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/SSD MultiBox.pdf:pdf
      \endverb
      \keyw{Convolutional neural network,Real-time object detection}
    \endentry
    \entry{Redmon2016}{article}{}
      \name{author}{2}{}{%
        {{hash=99bced2e56a5253f3fe98a5f04e6d9b2}{%
           family={Redmon},
           family_i={R\bibinitperiod},
           given={Joseph},
           given_i={J\bibinitperiod}}}%
        {{hash=396c6ddedb6f986906fc3e4994d19974}{%
           family={Farhadi},
           family_i={F\bibinitperiod},
           given={Ali},
           given_i={A\bibinitperiod}}}%
      }
      \strng{namehash}{e3b480c56f52be95ad68ce67e9708174}
      \strng{fullhash}{e3b480c56f52be95ad68ce67e9708174}
      \field{sortinit}{2}
      \field{sortinithash}{8343b463aacf48517c044b4d2c9c45ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.}
      \field{eprinttype}{arXiv}
      \field{isbn}{978-1-5386-0457-1}
      \field{issn}{0146-4833}
      \field{title}{{YOLO9000: Better, Faster, Stronger}}
      \field{year}{2016}
      \verb{doi}
      \verb 10.1109/CVPR.2017.690
      \endverb
      \verb{eprint}
      \verb 1612.08242
      \endverb
      \verb{file}
      \verb :C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/YOLOv2.pdf:pdf
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1612.08242
      \endverb
    \endentry
  \endsortlist
\endrefsection
\endinput

