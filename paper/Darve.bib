
@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/YOLOv2.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {0146-4833},
pmid = {23021419},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@article{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/SSD MultiBox.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
pages = {21--37},
pmid = {23739795},
title = {{SSD: Single shot multibox detector}},
volume = {9905 LNCS},
year = {2016}
}
@article{V.Oseledets2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.5877},
author = {{V. Oseledets}},
doi = {10.1137/09076756X},
eprint = {arXiv:1302.5877},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Tensor Train Decomposition.pdf:pdf},
isbn = {0001405101},
journal = {SIAM J. Scientific Computing},
keywords = {090761811,10,1137,62k20,62l05,62p30,65d05,active learning,ams subject classifications,doi,experimental design,local linear approximation,nonlinear function approximation,sequential design},
number = {5},
pages = {1948--1974},
title = {{Tensor-Train Decomposition}},
volume = {33},
year = {2011}
}
@article{Novikov2015,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {1509.06569},
author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
eprint = {1509.06569},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Tensorizing Neural Networks.pdf:pdf},
issn = {10495258},
pages = {1--9},
pmid = {1969590},
title = {{Tensorizing Neural Networks}},
url = {http://arxiv.org/abs/1509.06569},
year = {2015}
}
@article{Corona2017,
abstract = {We present a framework using the Quantized Tensor Train (QTT) decomposition to accurately and efficiently solve volume and boundary integral equations in three dimensions. We describe how the QTT decomposition can be used as a hierarchical compression and inversion scheme for matrices arising from the discretization of integral equations. For a broad range of problems, computational and storage costs of the inversion scheme are extremely modest O(log⁡N) and once the inverse is computed, it can be applied in O(Nlog⁡N). We analyze the QTT ranks for hierarchically low rank matrices and discuss its relationship to commonly used hierarchical compression techniques such as FMM and HSS. We prove that the QTT ranks are bounded for translation-invariant systems and argue that this behavior extends to non-translation invariant volume and boundary integrals. For volume integrals, the QTT decomposition provides an efficient direct solver requiring significantly less memory compared to other fast direct solvers. We present results demonstrating the remarkable performance of the QTT-based solver when applied to both translation and non-translation invariant volume integrals in 3D. For boundary integral equations, we demonstrate that using a QTT decomposition to construct preconditioners for a Krylov subspace method leads to an efficient and robust solver with a small memory footprint. We test the QTT preconditioners in the iterative solution of an exterior elliptic boundary value problem (Laplace) formulated as a boundary integral equation in complex, multiply connected geometries.},
archivePrefix = {arXiv},
arxivId = {1511.06029},
author = {Corona, Eduardo and Rahimian, Abtin and Zorin, Denis},
doi = {10.1016/j.jcp.2016.12.051},
eprint = {1511.06029},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Tensor Train Integral Equations.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Complex geometries,Fast multipole methods,Hierarchical matrix compression and inversion,Integral equations,Preconditioned iterative solver,Tensor Train decomposition},
pages = {145--169},
publisher = {Elsevier Inc.},
title = {{A Tensor-Train accelerated solver for integral equations in complex geometries}},
url = {http://dx.doi.org/10.1016/j.jcp.2016.12.051},
volume = {334},
year = {2017}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/YOLO.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01689002},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Adler2014,
abstract = {This research investigates the traffic police routine patrol vehicle (RPV) assign- ment problem on an interurban road network through a series of integer linear programs. The traffic police RPV's main task, like other emergency services, is to handle calls-for- service. Emergency services allocation models are generally based on the shortest path al- gorithm however, the traffic police RPV also handles other roles, namely patrolling to create a presence that acts as a deterrence, and issuing tickets to offenders. The RPVs need to be located dynamically on both hazardous sections and on roads with heavy traffic in order to increase their presence and conspicuousness, in an attempt to prevent or reduce traffic offences, road accidents and traffic congestion. Due to the importance of the traffic patrol vehicle's location with regard to their additional roles, allocation of the RPVs adheres to an exogenous, legal, time-to-arrival constraint. We develop location-allocation models and apply them to a case study of the road network in northern Israel. The results of the four models are compared to each other and in relation to the current chosen locations. The multiple formulations provide alternatives that jointly account for road safety and policing objectives which aid decision-makers in the selection of their preferred RPV assignments. The results of the models present a location-allocation configuration per RPV per shift with full call-for-service coverage whilst maximizing police presence and conspicuousness as a proxy for road safety.},
author = {Adler, Nicole and Hakkert, Alfred Shalom and Kornbluth, Jonathan and Raviv, Tal and Sher, Mali},
doi = {10.1007/s10479-012-1275-2},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Location-allocation-for-traffice-police-patrol.pdf:pdf},
isbn = {0254-5330},
issn = {15729338},
journal = {Annals of Operations Research},
keywords = {Allocation,Emergency services,Location,Network,Traffic police},
number = {1},
pages = {9--31},
title = {{Location-allocation models for traffic police patrol vehicles on an interurban network}},
volume = {221},
year = {2014}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/VGG16.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Han2015,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J.},
doi = {abs/1510.00149/1510.00149},
eprint = {1510.00149},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Han Mao Dally 2016.pdf:pdf},
pages = {1--14},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
url = {http://arxiv.org/abs/1510.00149},
year = {2015}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:C$\backslash$:/Users/bjack/Google Drive/Grad School/Research/Articles/Fast{\_}RCNN.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}

@ARTICLE{KITTIDataset,
	author = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun},
	title = {Vision meets Robotics: The KITTI Dataset},
	journal = {International Journal of Robotics Research (IJRR)},
	year = {2013}
}

@INPROCEEDINGS{KITTIVision,
	author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
	title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
	booktitle = {Conference on Computer Vision and Pattern	Recognition (CVPR)},
	year = {2012}
}
